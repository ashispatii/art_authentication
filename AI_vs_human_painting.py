# -*- coding: utf-8 -*-
"""Ankita_AI_Artbench_AI_vs_Human_art_detection_(90_+_acc)_hyperpTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGmFIlw632E6iY_g28u1Xe9tUmDdFZ_Z

#Basics
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
ravidussilva_real_ai_art_path = kagglehub.dataset_download('ravidussilva/real-ai-art')

print('Data source import complete.')

"""**Initialization**

The notebook has been tested using GPU T4 x2
"""

!pip install keras-tuner
import os
import random
from matplotlib import pyplot as plt
import cv2

import numpy as np
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.utils import image_dataset_from_directory
from keras.models import Sequential
from keras.layers import Rescaling, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Dropout
from keras.metrics import Precision, Recall
import keras_tuner as kt
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

"""**Listing input train and test data**"""

top_dir = '/kaggle/input/real-ai-art/Real_AI_SD_LD_Dataset'

# Define the training paths
train_dir = os.path.join(top_dir, 'train')

# List all directories in the train directory
all_directories  =  os.listdir(train_dir)

# Initialize lists to store directories for human-drawn and AI-generated images
train_human = []
train_ai = []

# Loop through all directories
for directory in all_directories:
    # Check if the directory represents human-drawn images
    if not directory.startswith('AI_'):
        train_human.append(os.path.join(train_dir, directory))
    # Check if the directory represents AI-generated images
    else:
        train_ai.append(os.path.join(train_dir, directory))

# Print the lists of directories
print("Train directories containing human-drawn images:")
for i, directory in enumerate(train_human):
    print(f"{i}. {directory}")

print("\nTrain directories containing AI-generated images:")
for i, directory in enumerate(train_ai):
    print(f"{i}. {directory}")

# Define the test paths
test_dir = os.path.join(top_dir, 'test')

# List all directories in the test directory
all_directories = os.listdir(test_dir)

# Initialize lists to store directories for human-drawn and AI-generated images
test_human = []
test_ai = []

# Loop through all directories
for directory in all_directories:
    # Check if the directory represents human-drawn images
    if not directory.startswith('AI_'):
        test_human.append(os.path.join(test_dir, directory))
    # Check if the directory represents AI-generated images
    else:
        test_ai.append(os.path.join(test_dir, directory))

# Print the lists of directories
print("Test directories containing human-drawn images:")
for i, directory in enumerate(test_human):
    print(f"{i}. {directory}")

print("\nTest directories containing AI-generated images:")
for i, directory in enumerate(test_ai):
    print(f"{i}. {directory}")

"""**Dataset analysis**"""

# Plot k-number of images from the dataset
def plot_im(directory, k):
    files = os.listdir(directory)
    im = random.choices(files, k=k)

    fig = plt.figure()

    for i in range(k):
        im_i_path = os.path.join(directory, im[i])  # File path
        im_i = cv2.imread(im_i_path)

        # Add subplot
        ax = fig.add_subplot(int(np.sqrt(k)), int(np.sqrt(k)), i + 1)

        # Plot image
        ax.imshow(im_i)
        ax.axis('off')

        # Display filename below the image
        ax.set_title(im[i], fontsize=8, pad=2)

    plt.tight_layout()  # Adjust layout
    plt.show()

# Visualize random images from train_human. Catagory is sorted in order of output in cell 2
real_im = plot_im(directory=train_human[7], k=9)
plt.show()

# Visualize random images from train_ai. Catagory is sorted in order of output in cell 2
ai_im = plot_im(directory=train_ai[4], k=9)
plt.show()

"""**Setting up train and test set under labels "human" and "AI"**

**Labelling the training set**
"""

# Initialize lists to store file paths and labels
filepaths = []
labels = []

# Initialize an empty DataFrame for train_data
train_data = pd.DataFrame(columns=['filepath', 'label'])

# Label files under train_human as "human"
for directory in train_human:
    for file in os.listdir(directory):
        filepath = os.path.join(directory, file)
        filepaths.append(filepath)
        labels.append("human")

# Label files under train_ai as "AI"
for directory in train_ai:
    for file in os.listdir(directory):
        filepath = os.path.join(directory, file)
        filepaths.append(filepath)
        labels.append("AI")

# Create a DataFrame with file paths and labels
data = pd.DataFrame({'filepath': filepaths, 'label': labels})

# Concatenate data with train_data
train_data = pd.concat([train_data, data], ignore_index=True)

# Display the first few rows of the train_data DataFrame
print(train_data.head())

# Count the number of files under each label
file_counts = train_data['label'].value_counts()

# Print the counts
print("Number of files under each label:")
print(file_counts)

"""**Reducing AI data to balance-off**"""

# Set the random seed for reproducibility
random_seed = 123  # Change this to your desired seed number
np.random.seed(random_seed)

# Number of rows to drop for the 'AI' label


# Get the indices of rows with the 'AI' label
ai_indices = train_data[train_data['label'] == 'AI'].index
human_indices = train_data[train_data['label'] == 'human'].index


# Randomly select indices to drop using the specified seed
indices_to_drop = np.random.choice(ai_indices, 94015, replace=False)

# Drop the selected rows from the DataFrame
train_data = train_data.drop(indices_to_drop)
indices_to_drop2 = np.random.choice(human_indices, 39000, replace=False)

# Drop the selected rows from the DataFrame
train_data = train_data.drop(indices_to_drop2)


# Reset the index of the DataFrame after dropping rows
train_data.reset_index(drop=True, inplace=True)

# Count the number of files under each label
file_counts = train_data['label'].value_counts()

# Print the counts
print("Number of files under each label:")
print(file_counts)

from sklearn.model_selection import train_test_split

# Assume df is your DataFrame
train_data,val_data= train_test_split(train_data, test_size=0.2, random_state=42)
# Count the number of files under each label
file_counts = train_data['label'].value_counts()

# Print the counts
print("Number of files under each label:")
print(file_counts)
# Count the number of files under each label
file_counts = val_data['label'].value_counts()

# Print the counts
print("Number of files under each label:")
print(file_counts)

# Display the first few rows of the train_data DataFrame
print(train_data.head())

# Count the number of files under each label
file_counts = train_data['label'].value_counts()

# Print the counts
print("\nNumber of files under each label:")
print(file_counts)

"""**Labelling the test set**"""

# Initialize lists to store file paths and labels
filepaths = []
labels = []

# Initialize an empty DataFrame for test_data
test_data = pd.DataFrame(columns=['filepath', 'label'])

# Label files under test_human as "human"
for directory in test_human:
    for file in os.listdir(directory):
        filepath = os.path.join(directory, file)
        filepaths.append(filepath)
        labels.append("human")

# Label files under test_ai as "AI"
for directory in test_ai:
    for file in os.listdir(directory):
        filepath = os.path.join(directory, file)
        filepaths.append(filepath)
        labels.append("AI")

# Create a DataFrame with file paths and labels
data = pd.DataFrame({'filepath': filepaths, 'label': labels})

# Concatenate data with test_data
test_data = pd.concat([test_data, data], ignore_index=True)

# Set the random seed for reproducibility
random_seed = 123  # Change this to your desired seed number
np.random.seed(random_seed)

# Number of rows to drop for the 'AI' label
num_to_drop = 40000

# Get the indices of rows with the 'AI' label
ai_indices = test_data[test_data['label'] == 'AI'].index
human_indices = test_data[test_data['label'] == 'human'].index


# Randomly select indices to drop using the specified seed
indices_to_drop = np.random.choice(ai_indices, 19000, replace=False)

# Drop the selected rows from the DataFrame
test_data = test_data.drop(indices_to_drop)
indices_to_drop2 = np.random.choice(human_indices, 9000, replace=False)

# Drop the selected rows from the DataFrame
test_data = test_data.drop(indices_to_drop2)


# Reset the index of the DataFrame after dropping rows
test_data.reset_index(drop=True, inplace=True)

# Count the number of files under each label
file_counts = test_data['label'].value_counts()

# Print the counts
print("Number of files under each label:")
print(file_counts)

# Display the first few rows of the test_data DataFrame
print(test_data.head())

# Count the number of files under each label
file_counts = test_data['label'].value_counts()

# Print the counts
print("\nNumber of files under each label:")
print(file_counts)

"""**Creating the train set**"""

training_generator = ImageDataGenerator(rescale=1./255,   # to normalize pixel value
                                       # rotation_range=7, # it will apply rotations to the image
                                       # horizontal_flip=True, # it will flip image horizontally
                                       # zoom_range=0.2  # it will increase and decrease zoom by 0.2x
                                       )
train_dataset = training_generator.flow_from_dataframe(
    dataframe=train_data,
    x_col='filepath',  # Column containing file paths
    y_col='label',     # Column containing labels
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=True
)

"""**Creating the validation set**"""

training_generator = ImageDataGenerator(rescale=1./255,   # to normalize pixel value
                                       # rotation_range=7, # it will apply rotations to the image
                                       # horizontal_flip=True, # it will flip image horizontally
                                       # zoom_range=0.2  # it will increase and decrease zoom by 0.2x
                                       )
val_dataset = training_generator.flow_from_dataframe(
    dataframe=val_data,
    x_col='filepath',  # Column containing file paths
    y_col='label',     # Column containing labels
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=True
)

"""**Creating the test set**"""

test_generator = ImageDataGenerator(rescale=1./255)
test_dataset = test_generator.flow_from_dataframe(  dataframe=test_data,
                                                    x_col='filepath',  # Column containing file paths
                                                    y_col='label',     # Column containing labels
                                                    target_size = (224, 224),
                                                    batch_size = 1,    # 1 image at a time to evaluate the NN
                                                    class_mode = 'categorical',
                                                    shuffle = False)   # to associate the prediction with expected output

test_dataset.class_indices

"""# Kaggle model Building the neural network"""

network = Sequential()
network.add(Conv2D(filters = 64, kernel_size = 3, input_shape = (32,32,3), activation='relu'))
network.add(MaxPooling2D())
network.add(Conv2D(filters = 64, kernel_size = 3, activation='relu'))
network.add(MaxPooling2D())
network.add(Flatten())
network.add(Dense(units = 64, activation='relu'))
network.add(Dense(units = 2, activation='softmax'))

network.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
network.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""**Training the neural network**"""

import time

# Initial epoch count
epochs = 15

# Initialize total training time
total_training_time = 0

# Loop to allow user to continue training
while True:
    # Record start time
    start_time = time.time()

    # Fit the model for the specified number of epochs
    history = network.fit(train_dataset, epochs=epochs, validation_data=test_dataset, callbacks=[model_checkpoint])

    # Calculate training time for the current batch of epochs
    training_time = time.time() - start_time
    total_training_time += training_time

    # Print total training time
    print("Total training time so far: {:.2f} seconds".format(total_training_time))

    # Prompt the user to continue training or stop
    user_input = input("Enter 'c' to continue training for more epochs, or any other key to stop: ")

    # If user input is not 'c', break out of the loop
    if user_input != 'c':
        break

    # Ask the user for the number of additional epochs
    additional_epochs = int(input("Enter the number of additional epochs you want to train for: "))

    # Update the epoch count for the next training loop
    epochs += additional_epochs

"""**Evaluating the network**"""

history.history.keys()

plt.plot(history.history['val_loss']);
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.title('Validation Loss over Epochs')
plt.show()

plt.plot(history.history['val_accuracy']);
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy over Epochs')
plt.show()

predictions = network.predict(test_dataset)
predictions

test_dataset.class_indices

predictions = np.argmax(predictions, axis = 1)
predictions

#import seaborn as sns
#from sklearn.metrics import confusion_matrix, classification_report
#cm = confusion_matrix(test_dataset.classes, predictions)
#sns.heatmap(cm, annot=True)
#print(classification_report(test_dataset.classes, predictions))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Calculate confusion matrix and classification report
cm = confusion_matrix(test_dataset.classes, predictions)
report = classification_report(test_dataset.classes, predictions)

# Define custom labels for the axes
labels = ['AI', 'Human']
cm_df = pd.DataFrame(cm, index=labels, columns=labels)

# Plot confusion matrix using Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')

# Add labels to the axes
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')

# Print classification report
print(report)

# Show the plot
plt.show()

"""**Optional: Saving the model network and weights seperately**"""

model_json = network.to_json()
with open('network.json','w') as json_file:
  json_file.write(model_json) #save network json

from keras.models import save_model
network_saved = save_model(network, '/kaggle/working/weights.hdf5')

"""#*Pretrained model*

#**VGG19**
"""

from keras.applications import VGG19
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Input

base_model = VGG19(include_top=False, weights='imagenet', input_shape=(32, 32, 3))
base_model.trainable = False

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""VGG19_Trainable_true"""

from keras.applications import VGG19
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Input
from keras.callbacks import ModelCheckpoint

base_model = VGG19(include_top=False, weights='imagenet', input_shape=(32, 32, 3))
base_model.trainable = True  # Freeze base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**InceptionResNetV2**"""

from keras.applications import InceptionResNetV2
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

base_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base layers

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))  # For multi-class; use 1 & sigmoid for binary
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""InceptionResNetV2_Trainable_true"""

from keras.applications import InceptionResNetV2
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

base_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**Xception**"""

from keras.applications import Xception
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

base_model = Xception(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""Xception_Trainable_true"""

from keras.applications import Xception
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

base_model = Xception(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**DenseNet**"""

from keras.applications import DenseNet201
from keras.models import Sequential
from keras.layers import Dense, Flatten, GlobalAveragePooling2D
from keras.layers import Input
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base layers

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(GlobalAveragePooling2D())  # Better than Flatten for ResNet
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""DenseNet_Trainable_true"""

from keras.applications import DenseNet201
from keras.models import Sequential
from keras.layers import Dense, Flatten, GlobalAveragePooling2D
from keras.layers import Input
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

# Load ResNet50 base model without the top classification layer
base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True  # Freeze the base layers

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding ='same'))
model.add(GlobalAveragePooling2D())  # Better than Flatten for ResNet
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**EfficientNetB7**"""

from keras.applications import EfficientNetB7
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

# Load EfficientNetB7 without the top classification layer
base_model = EfficientNetB7(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))  # Use 1 & sigmoid for binary
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""EfficientNetB7_Trainable_true"""

from keras.applications import EfficientNetB7
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

# Load EfficientNetB7 without the top classification layer
base_model = EfficientNetB7(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True  # Freeze the base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**MobileNetV2**"""

from keras.applications import MobileNetV2
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

# Load EfficientNetB7 without the top classification layer
base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))  # Use 1 & sigmoid for binary
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""MobileNetV2_trainable_true"""

from keras.applications import MobileNetV2
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint

# Load EfficientNetB7 without the top classification layer
base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True  # Freeze the base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))  # Use 1 & sigmoid for binary
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#*Petrained with attention*

#**MobileNetV2_trainable_true**
"""

from keras.applications import MobileNetV2
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Input

base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True  # Freeze base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import models, optimizers
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Activation, Dropout,
                                     GlobalAveragePooling2D, Dense, Input, Multiply,
                                     GlobalMaxPooling2D, Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input
input_layer = Input(shape=(224, 224, 3))
base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = True
for layer in base_model.layers[:100]:
    layer.trainable = False

x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

from keras.applications import MobileNetV2
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Input

base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import models, optimizers
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Activation, Dropout,
                                     GlobalAveragePooling2D, Dense, Input, Multiply,
                                     GlobalMaxPooling2D, Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input
input_layer = Input(shape=(224, 224, 3))
base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = False
for layer in base_model.layers[:100]:
    layer.trainable = False

x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**InceptionResNetV2_trainable_true**"""

from keras.applications import InceptionResNetV2
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

base_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True  # Freeze the base layers

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras import models, optimizers
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Activation, Dropout,
                                     GlobalAveragePooling2D, Dense, Input, Multiply,
                                     GlobalMaxPooling2D, Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input - minimum recommended size for InceptionResNetV2 is 75x75
input_layer = Input(shape=(224, 224, 3))
base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = True
# Freeze initial layers if needed
for layer in base_model.layers[:100]:
    layer.trainable = False

# Add custom layers
x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Build and summarize model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

from keras.applications import InceptionResNetV2
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

base_model = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False # Freeze the base layers

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras import models, optimizers
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Activation, Dropout,
                                     GlobalAveragePooling2D, Dense, Input, Multiply,
                                     GlobalMaxPooling2D, Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input - minimum recommended size for InceptionResNetV2 is 75x75
input_layer = Input(shape=(224, 224, 3))
base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = False
# Freeze initial layers if needed
for layer in base_model.layers[:100]:
    layer.trainable = False

# Add custom layers
x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Build and summarize model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**Xception_trainable_true**"""

from keras.applications import Xception
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

base_model = Xception(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))  # For multi-class; use 1 & sigmoid for binary

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import Xception
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input - suitable size for Xception
input_layer = Input(shape=(224, 224, 3))
base_model = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = True

# Optionally freeze some layers
for layer in base_model.layers[:100]:
    layer.trainable = False

# Custom top layers with SE and spatial attention
x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Final model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**Xception_trainable_false**"""

from keras.applications import Xception
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

base_model = Xception(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))  # For multi-class; use 1 & sigmoid for binary

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import Xception
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input - suitable size for Xception
input_layer = Input(shape=(224, 224, 3))
base_model = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = False

# Optionally freeze some layers
for layer in base_model.layers[:100]:
    layer.trainable = False

# Custom top layers with SE and spatial attention
x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Final model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**DenseNet201_trainable_true**"""

from keras.applications import DenseNet201
from keras.models import Sequential
from keras.layers import Dense, Flatten, GlobalAveragePooling2D
from keras.layers import Input
from keras.optimizers import Adam


base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import DenseNet201
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input layer
input_layer = Input(shape=(224, 224, 3))  # You can go as low as (32, 32, 3) but larger is better
base_model = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = True

# Optionally freeze initial layers
for layer in base_model.layers[:100]:
    layer.trainable = False

# Custom top layers
x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Define model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**DenseNet201_trainable_false**"""

from keras.applications import DenseNet201
from keras.models import Sequential
from keras.layers import Dense, Flatten, GlobalAveragePooling2D
from keras.layers import Input
from keras.optimizers import Adam


base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False

# Build the model
model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import DenseNet201
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input layer
input_layer = Input(shape=(224, 224, 3))  # You can go as low as (32, 32, 3) but larger is better
base_model = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = False

# Optionally freeze initial layers
for layer in base_model.layers[:100]:
    layer.trainable = False

# Custom top layers
x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Define model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**EfficientNetB7_trainable_true**"""

from keras.applications import EfficientNetB7
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

# Load EfficientNetB7 without the top classification layer
base_model = EfficientNetB7(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = True  # Freeze the base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))  # Use 1 & sigmoid for binary
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import EfficientNetB7
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input layer - EfficientNetB7 requires 600x600 minimum
input_layer = Input(shape=(224, 224, 3))
base_model = EfficientNetB7(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = True

# Optionally freeze some initial layers
for layer in base_model.layers[:100]:
    layer.trainable = False

# Add custom layers
x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Final model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#**EfficientNetB7_trainable_false**"""

from keras.applications import EfficientNetB7
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

# Load EfficientNetB7 without the top classification layer
base_model = EfficientNetB7(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base layers

model = Sequential()
model.add(base_model)
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='softmax'))  # Use 1 & sigmoid for binary
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import EfficientNetB7
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input layer - EfficientNetB7 requires 600x600 minimum
input_layer = Input(shape=(224, 224, 3))
base_model = EfficientNetB7(weights='imagenet', include_top=False, input_tensor=input_layer)
base_model.trainable = False

# Optionally freeze some initial layers
for layer in base_model.layers[:100]:
    layer.trainable = False

# Add custom layers
x = base_model.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Final model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#Simple Concatenation Strategy"""

from tensorflow.keras.applications import DenseNet201, Xception
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply, concatenate,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Input


# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])


# Input layer
input_layer = Input(shape=(224, 224, 3))

# Xception branch
xception = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
xception.trainable = True
for layer in xception.layers[:100]:
    layer.trainable = False
x = xception.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)  # Make sure you have these defined
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)

# DenseNet201 branch
densenet = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
densenet.trainable = True
for layer in densenet.layers[:100]:
    layer.trainable = False
y = densenet.output

y = Conv2D(32, (3, 3), padding='same')(y)
y = BatchNormalization()(y)
y = squeeze_excite_block(y)
y = spatial_attention_block(y)
y = GlobalAveragePooling2D()(y)

# Concatenate features from both models
combined = concatenate([x, y])

# Final layers
z = Dense(256, activation='relu')(combined)
output = Dense(2, activation='softmax')(z)

# Create the combined model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))


from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
predictions = model.predict(test_dataset)
predictions = np.argmax(predictions, axis = 1)

# Calculate confusion matrix and classification report
cm = confusion_matrix(test_dataset.classes, predictions)
report = classification_report(test_dataset.classes, predictions)

# Define custom labels for the axes
labels = ['AI', 'Human']
cm_df = pd.DataFrame(cm, index=labels, columns=labels)

# Plot confusion matrix using Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')

# Add labels to the axes
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')

# Print classification report
print(report)

# Show the plot
plt.show()

history.history.keys()
plt.plot(history.history['val_loss']);
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.title('Validation Loss over Epochs')
plt.show()

plt.plot(history.history['val_accuracy']);
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy over Epochs')
plt.show()

!pip install matplotlib scikit-learn
from sklearn.metrics import roc_curve, auc, RocCurveDisplay
import matplotlib.pyplot as plt
import numpy as np

# Get predicted probabilities for the test set
y_pred_probs = model.predict(test_dataset)

# Convert true labels to numpy array
y_true = np.array(test_dataset.classes)

# Calculate ROC curve and AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

# For binary classification
fpr["AI"], tpr["AI"], _ = roc_curve(y_true, y_pred_probs[:, 0], pos_label=0)
roc_auc["AI"] = auc(fpr["AI"], tpr["AI"])

fpr["Human"], tpr["Human"], _ = roc_curve(y_true, y_pred_probs[:, 1], pos_label=1)
roc_auc["Human"] = auc(fpr["Human"], tpr["Human"])

# Plot ROC curves
plt.figure(figsize=(6, 5))

# Plot ROC for AI class
plt.plot(fpr["AI"], tpr["AI"], color='blue', lw=2,
         label='ROC curve for AI (AUC = %0.2f)' % roc_auc["AI"])

# Plot ROC for Human class
plt.plot(fpr["Human"], tpr["Human"], color='green', lw=2,
         label='ROC curve for Human (AUC = %0.2f)' % roc_auc["Human"])

# Plot diagonal line
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')

# Set plot parameters
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")

# Show plot
plt.show()

# Print AUC values
print(f"AUC for AI class: {roc_auc['AI']:.4f}")
print(f"AUC for Human class: {roc_auc['Human']:.4f}")

# Calculate micro-average AUC (only needed for multi-class)
# Remove this section if you're doing binary classification
try:
    fpr["micro"], tpr["micro"], _ = roc_curve(y_true, y_pred_probs.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    print(f"Micro-average AUC: {roc_auc['micro']:.4f}")
except Exception as e:
    print(f"Skipping micro-average calculation (binary classification): {str(e)}")

"""#XAI on Hybrid Model

GRAD-CAM
"""

import tensorflow as tf
for i, layer in enumerate(model.layers):
    if isinstance(layer, tf.keras.layers.Conv2D):
        print(f"{i}: {layer.name}")

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

    from tensorflow.keras.preprocessing import image
import numpy as np

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = tf.keras.applications.densenet.preprocess_input(np.expand_dims(img_array, axis=0))
    return img_array, img

img_path = "/content/B1.jpg"
img_array, original_img = preprocess_image(img_path)

# Replace 'conv2d_1' with your last shared Conv2D layer before concatenation
from tensorflow.keras.models import Model
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name='conv2d_1')

import cv2
import matplotlib.pyplot as plt

def superimpose_heatmap(heatmap, original_img, alpha=0.4):
    img = np.array(original_img)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed = heatmap_color * alpha + img
    return np.uint8(superimposed)

superimposed_img = superimpose_heatmap(heatmap, original_img)

plt.imshow(superimposed_img)
plt.title("Grad-CAM on Hybrid Model")
plt.axis('off')
plt.show()

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

    from tensorflow.keras.preprocessing import image
import numpy as np

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = tf.keras.applications.densenet.preprocess_input(np.expand_dims(img_array, axis=0))
    return img_array, img

img_path = "/content/R1.jpg"
img_array, original_img = preprocess_image(img_path)

# Replace 'conv2d_1' with your last shared Conv2D layer before concatenation
from tensorflow.keras.models import Model
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name='conv2d_1')

import cv2
import matplotlib.pyplot as plt

def superimpose_heatmap(heatmap, original_img, alpha=0.4):
    img = np.array(original_img)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed = heatmap_color * alpha + img
    return np.uint8(superimposed)

superimposed_img = superimpose_heatmap(heatmap, original_img)

plt.imshow(superimposed_img)
plt.title("Grad-CAM on Hybrid Model")
plt.axis('off')
plt.show()

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

    from tensorflow.keras.preprocessing import image
import numpy as np

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = tf.keras.applications.densenet.preprocess_input(np.expand_dims(img_array, axis=0))
    return img_array, img

img_path = "/content/U1.jpg"
img_array, original_img = preprocess_image(img_path)

# Replace 'conv2d_1' with your last shared Conv2D layer before concatenation
from tensorflow.keras.models import Model
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name='conv2d_1')

import cv2
import matplotlib.pyplot as plt

def superimpose_heatmap(heatmap, original_img, alpha=0.4):
    img = np.array(original_img)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed = heatmap_color * alpha + img
    return np.uint8(superimposed)

superimposed_img = superimpose_heatmap(heatmap, original_img)

plt.imshow(superimposed_img)
plt.title("Grad-CAM on Hybrid Model")
plt.axis('off')
plt.show()

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

    from tensorflow.keras.preprocessing import image
import numpy as np

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = tf.keras.applications.densenet.preprocess_input(np.expand_dims(img_array, axis=0))
    return img_array, img

img_path = "/content/abraham-storck_die-niederlandische-flotte-auf-der-reede-vor-amsterdam-0.jpg"
img_array, original_img = preprocess_image(img_path)

# Replace 'conv2d_1' with your last shared Conv2D layer before concatenation
from tensorflow.keras.models import Model
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name='conv2d_1')

import cv2
import matplotlib.pyplot as plt

def superimpose_heatmap(heatmap, original_img, alpha=0.4):
    img = np.array(original_img)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed = heatmap_color * alpha + img
    return np.uint8(superimposed)

superimposed_img = superimpose_heatmap(heatmap, original_img)

plt.imshow(superimposed_img)
plt.title("Grad-CAM on Hybrid Model")
plt.axis('off')
plt.show()

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

    from tensorflow.keras.preprocessing import image
import numpy as np

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = tf.keras.applications.densenet.preprocess_input(np.expand_dims(img_array, axis=0))
    return img_array, img

img_path = "/content/adachi-ginko_90.jpg"
img_array, original_img = preprocess_image(img_path)

# Replace 'conv2d_1' with your last shared Conv2D layer before concatenation
from tensorflow.keras.models import Model
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name='conv2d_1')

import cv2
import matplotlib.pyplot as plt

def superimpose_heatmap(heatmap, original_img, alpha=0.4):
    img = np.array(original_img)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed = heatmap_color * alpha + img
    return np.uint8(superimposed)

superimposed_img = superimpose_heatmap(heatmap, original_img)

plt.imshow(superimposed_img)
plt.title("Grad-CAM on Hybrid Model")
plt.axis('off')
plt.show()

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

    from tensorflow.keras.preprocessing import image
import numpy as np

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_array = tf.keras.applications.densenet.preprocess_input(np.expand_dims(img_array, axis=0))
    return img_array, img

img_path = "/content/adolphe-joseph-thomas-monticelli_margaree-faust-and-mephisto.jpg"
img_array, original_img = preprocess_image(img_path)

# Replace 'conv2d_1' with your last shared Conv2D layer before concatenation
from tensorflow.keras.models import Model
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name='conv2d_1')

import cv2
import matplotlib.pyplot as plt

def superimpose_heatmap(heatmap, original_img, alpha=0.4):
    img = np.array(original_img)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
    superimposed = heatmap_color * alpha + img
    return np.uint8(superimposed)

superimposed_img = superimpose_heatmap(heatmap, original_img)

plt.imshow(superimposed_img)
plt.title("Grad-CAM on Hybrid Model")
plt.axis('off')
plt.show()

"""LIME"""

!pip install lime
from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
def predict_fn(images):
    images = tf.keras.applications.densenet.preprocess_input(images)
    return model.predict(images)
from tensorflow.keras.preprocessing import image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt

img_path = "/content/B1.jpg"
img = image.load_img(img_path, target_size=(224, 224))
img_np = image.img_to_array(img).astype(np.uint8)
explainer = lime_image.LimeImageExplainer()

explanation = explainer.explain_instance(
    img_np,
    predict_fn,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)
from skimage.segmentation import mark_boundaries

# Get explanation for the top class
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=5,
    hide_rest=False
)

plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.title("LIME Explanation (Top Features)")
plt.axis('off')
plt.show()

!pip install lime
from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
def predict_fn(images):
    images = tf.keras.applications.densenet.preprocess_input(images)
    return model.predict(images)
from tensorflow.keras.preprocessing import image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt

img_path = "/content/R1.jpg"
img = image.load_img(img_path, target_size=(224, 224))
img_np = image.img_to_array(img).astype(np.uint8)
explainer = lime_image.LimeImageExplainer()

explanation = explainer.explain_instance(
    img_np,
    predict_fn,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)
from skimage.segmentation import mark_boundaries

# Get explanation for the top class
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=5,
    hide_rest=False
)

plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.title("LIME Explanation (Top Features)")
plt.axis('off')
plt.show()

!pip install lime
from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
def predict_fn(images):
    images = tf.keras.applications.densenet.preprocess_input(images)
    return model.predict(images)
from tensorflow.keras.preprocessing import image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt

img_path = "/content/U1.jpg"
img = image.load_img(img_path, target_size=(224, 224))
img_np = image.img_to_array(img).astype(np.uint8)
explainer = lime_image.LimeImageExplainer()

explanation = explainer.explain_instance(
    img_np,
    predict_fn,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)
from skimage.segmentation import mark_boundaries

# Get explanation for the top class
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=5,
    hide_rest=False
)

plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.title("LIME Explanation (Top Features)")
plt.axis('off')
plt.show()

!pip install lime
from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
def predict_fn(images):
    images = tf.keras.applications.densenet.preprocess_input(images)
    return model.predict(images)
from tensorflow.keras.preprocessing import image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt

img_path = "/content/abraham-storck_die-niederlandische-flotte-auf-der-reede-vor-amsterdam-0.jpg"
img = image.load_img(img_path, target_size=(224, 224))
img_np = image.img_to_array(img).astype(np.uint8)
explainer = lime_image.LimeImageExplainer()

explanation = explainer.explain_instance(
    img_np,
    predict_fn,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)
from skimage.segmentation import mark_boundaries

# Get explanation for the top class
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=5,
    hide_rest=False
)

plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.title("LIME Explanation (Top Features)")
plt.axis('off')
plt.show()

!pip install lime
from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
def predict_fn(images):
    images = tf.keras.applications.densenet.preprocess_input(images)
    return model.predict(images)
from tensorflow.keras.preprocessing import image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt

img_path = "/content/adolphe-joseph-thomas-monticelli_margaree-faust-and-mephisto.jpg"
img = image.load_img(img_path, target_size=(224, 224))
img_np = image.img_to_array(img).astype(np.uint8)
explainer = lime_image.LimeImageExplainer()

explanation = explainer.explain_instance(
    img_np,
    predict_fn,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)
from skimage.segmentation import mark_boundaries

# Get explanation for the top class
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=5,
    hide_rest=False
)

plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.title("LIME Explanation (Top Features)")
plt.axis('off')
plt.show()

!pip install lime
from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
def predict_fn(images):
    images = tf.keras.applications.densenet.preprocess_input(images)
    return model.predict(images)
from tensorflow.keras.preprocessing import image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt

img_path = "/content/adachi-ginko_90.jpg"
img = image.load_img(img_path, target_size=(224, 224))
img_np = image.img_to_array(img).astype(np.uint8)
explainer = lime_image.LimeImageExplainer()

explanation = explainer.explain_instance(
    img_np,
    predict_fn,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)
from skimage.segmentation import mark_boundaries

# Get explanation for the top class
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=5,
    hide_rest=False
)

plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.title("LIME Explanation (Top Features)")
plt.axis('off')
plt.show()

"""#Changes after Hyperparameter Tuning

with experiment 1
"""

from tensorflow.keras.applications import DenseNet201, Xception
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply, concatenate,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input layer
input_layer = Input(shape=(224, 224, 3))

# Xception branch
xception = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
xception.trainable = True
for layer in xception.layers[:100]:
    layer.trainable = False
x = xception.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)

# DenseNet201 branch
densenet = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
densenet.trainable = True
for layer in densenet.layers[:100]:
    layer.trainable = False
y = densenet.output
y = Conv2D(32, (3, 3), padding='same')(y)
y = BatchNormalization()(y)
y = squeeze_excite_block(y)
y = spatial_attention_block(y)
y = GlobalAveragePooling2D()(y)

# Concatenate features from both models
combined = concatenate([x, y])

# Final classifier layers
z = Dense(128, activation='relu')(combined)
z = Dropout(0.3)(z)  # Added dropout for regularization
output = Dense(2, activation='softmax')(z)

# Create the model
model = models.Model(inputs=input_layer, outputs=output)

# Compile the model with Adam optimizer and low learning rate
optimizer = Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Define checkpoint callback
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Train the model
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

# Evaluate on test dataset
loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""with experiment 2"""

from tensorflow.keras.applications import DenseNet201, Xception
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply, concatenate,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import ModelCheckpoint

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input layer
input_layer = Input(shape=(224, 224, 3))

# Xception branch
xception = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
xception.trainable = True
for layer in xception.layers[:50]:  # Freeze only first 50 layers
    layer.trainable = False
x = xception.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)

# DenseNet201 branch
densenet = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
densenet.trainable = True
for layer in densenet.layers[:50]:  # Freeze only first 50 layers
    layer.trainable = False
y = densenet.output
y = Conv2D(32, (3, 3), padding='same')(y)
y = BatchNormalization()(y)
y = squeeze_excite_block(y)
y = spatial_attention_block(y)
y = GlobalAveragePooling2D()(y)

# Combine both branches
combined = concatenate([x, y])

# Final classifier
z = Dense(128, activation='relu')(combined)
z = Dropout(0.3)(z)
output = Dense(2, activation='softmax')(z)

# Create the model
model = models.Model(inputs=input_layer, outputs=output)

# Optimizer: SGD with momentum
optimizer = SGD(learning_rate=0.01, momentum=0.9)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Checkpoint callback
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Fit the model
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

# Evaluate on test set
loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""with experiment3"""

from tensorflow.keras.applications import DenseNet201, Xception
from tensorflow.keras import models
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply, concatenate,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.callbacks import ModelCheckpoint

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])

# Input layer
input_layer = Input(shape=(224, 224, 3))

# Xception branch
xception = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
xception.trainable = True
for layer in xception.layers[:50]:  # Freezing first 50 layers
    layer.trainable = False
x = xception.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)

# DenseNet201 branch
densenet = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
densenet.trainable = True
for layer in densenet.layers[:50]:
    layer.trainable = False
y = densenet.output
y = Conv2D(32, (3, 3), padding='same')(y)
y = BatchNormalization()(y)
y = squeeze_excite_block(y)
y = spatial_attention_block(y)
y = GlobalAveragePooling2D()(y)

# Combine both branches
combined = concatenate([x, y])

# ✨ Additional Dense Layers for classifier
z = Dense(128, activation='relu')(combined)
z = Dropout(0.3)(z)
z = Dense(64, activation='relu')(z)  # Additional dense layer
output = Dense(2, activation='softmax')(z)

# Create the model
model = models.Model(inputs=input_layer, outputs=output)

# Optimizer: RMSprop
optimizer = RMSprop(learning_rate=0.0005)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Checkpoint callback
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Train the model
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

# Evaluate the model
loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#Explainable AI

GRAD-CAM
"""

from google.colab import files
uploaded = files.upload()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
import cv2

#Load and Preprocess the Image
img_path = "B1.jpg"
def get_img_array(img_path, size=(224, 224)):
    img = image.load_img(img_path, target_size=size)
    array = image.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    array = tf.keras.applications.densenet.preprocess_input(array)
    return array, img

img_array, original_img = get_img_array(img_path)

#Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
#Apply Grad-CAM on Xception Branch
# Name of last conv layer in Xception branch
last_conv_xception = 'conv2d'  # Assuming it's named exactly like that after xception.output

heatmap_x = make_gradcam_heatmap(img_array, model, last_conv_xception)
#Apply Grad-CAM on DenseNet201 Branch
last_conv_densenet = 'conv2d_1'  # Adjust name if needed

heatmap_y = make_gradcam_heatmap(img_array, model, last_conv_densenet)

#Superimpose Heatmap on Image
def superimpose_heatmap(heatmap, original_image, alpha=0.4, colormap=cv2.COLORMAP_JET):
    img = np.array(original_image)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, colormap)
    superimposed_img = heatmap_color * alpha + img
    return np.uint8(superimposed_img)

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.title("Grad-CAM: Xception")
plt.imshow(superimpose_heatmap(heatmap_x, original_img))
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Grad-CAM: DenseNet201")
plt.imshow(superimpose_heatmap(heatmap_y, original_img))
plt.axis('off')

plt.tight_layout()
plt.show()

"""LIME"""

!pip install lime

from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
img_path = 'B1.jpg'

def preprocess_lime(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_np = image.img_to_array(img).astype('double')
    return img_np

image_np = preprocess_lime(img_path)

#Define a LIME-Compatible Prediction Function
def predict_fn(images):
    # Apply the same preprocessing as used for DenseNet/Xception
    images = tf.keras.applications.densenet.preprocess_input(images.copy())
    return model.predict(images)

#Create LIME Image Explainer
explainer = lime_image.LimeImageExplainer()
#Explain a Prediction
explanation = explainer.explain_instance(
    image_np.astype('uint8'),
    predict_fn,
    top_labels=2,
    hide_color=0,
    num_samples=1000  # you can increase for more accurate explanations
)
#Visualize the Explanation
from skimage.color import label2rgb

# Choose label index (e.g., 0 = Human, 1 = AI — adjust based on your dataset)
label_index = explanation.top_labels[0]  # or manually use 0 or 1

temp, mask = explanation.get_image_and_mask(
    label_index,
    positive_only=True,
    num_features=10,
    hide_rest=False
)

plt.figure(figsize=(6, 6))
plt.title('LIME Explanation')
plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.axis('off')
plt.show()

from google.colab import files
uploaded = files.upload()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
import cv2

#Load and Preprocess the Image
img_path = "R1.jpg"
def get_img_array(img_path, size=(224, 224)):
    img = image.load_img(img_path, target_size=size)
    array = image.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    array = tf.keras.applications.densenet.preprocess_input(array)
    return array, img

img_array, original_img = get_img_array(img_path)

#Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
#Apply Grad-CAM on Xception Branch
# Name of last conv layer in Xception branch
last_conv_xception = 'conv2d'  # Assuming it's named exactly like that after xception.output

heatmap_x = make_gradcam_heatmap(img_array, model, last_conv_xception)
#Apply Grad-CAM on DenseNet201 Branch
last_conv_densenet = 'conv2d_1'  # Adjust name if needed

heatmap_y = make_gradcam_heatmap(img_array, model, last_conv_densenet)

#Superimpose Heatmap on Image
def superimpose_heatmap(heatmap, original_image, alpha=0.4, colormap=cv2.COLORMAP_JET):
    img = np.array(original_image)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, colormap)
    superimposed_img = heatmap_color * alpha + img
    return np.uint8(superimposed_img)

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.title("Grad-CAM: Xception")
plt.imshow(superimpose_heatmap(heatmap_x, original_img))
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Grad-CAM: DenseNet201")
plt.imshow(superimpose_heatmap(heatmap_y, original_img))
plt.axis('off')

plt.tight_layout()
plt.show()

!pip install lime

from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
img_path = 'R1.jpg'

def preprocess_lime(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_np = image.img_to_array(img).astype('double')
    return img_np

image_np = preprocess_lime(img_path)

#Define a LIME-Compatible Prediction Function
def predict_fn(images):
    # Apply the same preprocessing as used for DenseNet/Xception
    images = tf.keras.applications.densenet.preprocess_input(images.copy())
    return model.predict(images)

#Create LIME Image Explainer
explainer = lime_image.LimeImageExplainer()
#Explain a Prediction
explanation = explainer.explain_instance(
    image_np.astype('uint8'),
    predict_fn,
    top_labels=2,
    hide_color=0,
    num_samples=1000  # you can increase for more accurate explanations
)
#Visualize the Explanation
from skimage.color import label2rgb

# Choose label index (e.g., 0 = Human, 1 = AI — adjust based on your dataset)
label_index = explanation.top_labels[0]  # or manually use 0 or 1

temp, mask = explanation.get_image_and_mask(
    label_index,
    positive_only=True,
    num_features=10,
    hide_rest=False
)

plt.figure(figsize=(6, 6))
plt.title('LIME Explanation')
plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.axis('off')
plt.show()

from google.colab import files
uploaded = files.upload()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
import cv2

#Load and Preprocess the Image
img_path = "U1.jpg"
def get_img_array(img_path, size=(224, 224)):
    img = image.load_img(img_path, target_size=size)
    array = image.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    array = tf.keras.applications.densenet.preprocess_input(array)
    return array, img

img_array, original_img = get_img_array(img_path)

#Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
#Apply Grad-CAM on Xception Branch
# Name of last conv layer in Xception branch
last_conv_xception = 'conv2d'  # Assuming it's named exactly like that after xception.output

heatmap_x = make_gradcam_heatmap(img_array, model, last_conv_xception)
#Apply Grad-CAM on DenseNet201 Branch
last_conv_densenet = 'conv2d_1'  # Adjust name if needed

heatmap_y = make_gradcam_heatmap(img_array, model, last_conv_densenet)

#Superimpose Heatmap on Image
def superimpose_heatmap(heatmap, original_image, alpha=0.4, colormap=cv2.COLORMAP_JET):
    img = np.array(original_image)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, colormap)
    superimposed_img = heatmap_color * alpha + img
    return np.uint8(superimposed_img)

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.title("Grad-CAM: Xception")
plt.imshow(superimpose_heatmap(heatmap_x, original_img))
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Grad-CAM: DenseNet201")
plt.imshow(superimpose_heatmap(heatmap_y, original_img))
plt.axis('off')

plt.tight_layout()
plt.show()

!pip install lime

from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
img_path = 'U1.jpg'

def preprocess_lime(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_np = image.img_to_array(img).astype('double')
    return img_np

image_np = preprocess_lime(img_path)

#Define a LIME-Compatible Prediction Function
def predict_fn(images):
    # Apply the same preprocessing as used for DenseNet/Xception
    images = tf.keras.applications.densenet.preprocess_input(images.copy())
    return model.predict(images)

#Create LIME Image Explainer
explainer = lime_image.LimeImageExplainer()
#Explain a Prediction
explanation = explainer.explain_instance(
    image_np.astype('uint8'),
    predict_fn,
    top_labels=2,
    hide_color=0,
    num_samples=1000  # you can increase for more accurate explanations
)
#Visualize the Explanation
from skimage.color import label2rgb

# Choose label index (e.g., 0 = Human, 1 = AI — adjust based on your dataset)
label_index = explanation.top_labels[0]  # or manually use 0 or 1

temp, mask = explanation.get_image_and_mask(
    label_index,
    positive_only=True,
    num_features=10,
    hide_rest=False
)

plt.figure(figsize=(6, 6))
plt.title('LIME Explanation')
plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.axis('off')
plt.show()

from google.colab import files
uploaded = files.upload()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
import cv2

#Load and Preprocess the Image
img_path = "abraham-storck_die-niederlandische-flotte-auf-der-reede-vor-amsterdam-0.jpg"
def get_img_array(img_path, size=(224, 224)):
    img = image.load_img(img_path, target_size=size)
    array = image.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    array = tf.keras.applications.densenet.preprocess_input(array)
    return array, img

img_array, original_img = get_img_array(img_path)

#Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
#Apply Grad-CAM on Xception Branch
# Name of last conv layer in Xception branch
last_conv_xception = 'conv2d'  # Assuming it's named exactly like that after xception.output

heatmap_x = make_gradcam_heatmap(img_array, model, last_conv_xception)
#Apply Grad-CAM on DenseNet201 Branch
last_conv_densenet = 'conv2d_1'  # Adjust name if needed

heatmap_y = make_gradcam_heatmap(img_array, model, last_conv_densenet)

#Superimpose Heatmap on Image
def superimpose_heatmap(heatmap, original_image, alpha=0.4, colormap=cv2.COLORMAP_JET):
    img = np.array(original_image)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, colormap)
    superimposed_img = heatmap_color * alpha + img
    return np.uint8(superimposed_img)

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.title("Grad-CAM: Xception")
plt.imshow(superimpose_heatmap(heatmap_x, original_img))
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Grad-CAM: DenseNet201")
plt.imshow(superimpose_heatmap(heatmap_y, original_img))
plt.axis('off')

plt.tight_layout()
plt.show()

!pip install lime

from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
img_path = 'abraham-storck_die-niederlandische-flotte-auf-der-reede-vor-amsterdam-0.jpg'

def preprocess_lime(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_np = image.img_to_array(img).astype('double')
    return img_np

image_np = preprocess_lime(img_path)

#Define a LIME-Compatible Prediction Function
def predict_fn(images):
    # Apply the same preprocessing as used for DenseNet/Xception
    images = tf.keras.applications.densenet.preprocess_input(images.copy())
    return model.predict(images)

#Create LIME Image Explainer
explainer = lime_image.LimeImageExplainer()
#Explain a Prediction
explanation = explainer.explain_instance(
    image_np.astype('uint8'),
    predict_fn,
    top_labels=2,
    hide_color=0,
    num_samples=1000  # you can increase for more accurate explanations
)
#Visualize the Explanation
from skimage.color import label2rgb

# Choose label index (e.g., 0 = Human, 1 = AI — adjust based on your dataset)
label_index = explanation.top_labels[0]  # or manually use 0 or 1

temp, mask = explanation.get_image_and_mask(
    label_index,
    positive_only=True,
    num_features=10,
    hide_rest=False
)

plt.figure(figsize=(6, 6))
plt.title('LIME Explanation')
plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.axis('off')
plt.show()

from google.colab import files
uploaded = files.upload()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
import cv2

#Load and Preprocess the Image
img_path = "adolphe-joseph-thomas-monticelli_margaree-faust-and-mephisto.jpg"
def get_img_array(img_path, size=(224, 224)):
    img = image.load_img(img_path, target_size=size)
    array = image.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    array = tf.keras.applications.densenet.preprocess_input(array)
    return array, img

img_array, original_img = get_img_array(img_path)

#Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
#Apply Grad-CAM on Xception Branch
# Name of last conv layer in Xception branch
last_conv_xception = 'conv2d'  # Assuming it's named exactly like that after xception.output

heatmap_x = make_gradcam_heatmap(img_array, model, last_conv_xception)
#Apply Grad-CAM on DenseNet201 Branch
last_conv_densenet = 'conv2d_1'  # Adjust name if needed

heatmap_y = make_gradcam_heatmap(img_array, model, last_conv_densenet)

#Superimpose Heatmap on Image
def superimpose_heatmap(heatmap, original_image, alpha=0.4, colormap=cv2.COLORMAP_JET):
    img = np.array(original_image)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, colormap)
    superimposed_img = heatmap_color * alpha + img
    return np.uint8(superimposed_img)

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.title("Grad-CAM: Xception")
plt.imshow(superimpose_heatmap(heatmap_x, original_img))
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Grad-CAM: DenseNet201")
plt.imshow(superimpose_heatmap(heatmap_y, original_img))
plt.axis('off')

plt.tight_layout()
plt.show()

!pip install lime

from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
img_path = 'adolphe-joseph-thomas-monticelli_margaree-faust-and-mephisto.jpg'

def preprocess_lime(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_np = image.img_to_array(img).astype('double')
    return img_np

image_np = preprocess_lime(img_path)

#Define a LIME-Compatible Prediction Function
def predict_fn(images):
    # Apply the same preprocessing as used for DenseNet/Xception
    images = tf.keras.applications.densenet.preprocess_input(images.copy())
    return model.predict(images)

#Create LIME Image Explainer
explainer = lime_image.LimeImageExplainer()
#Explain a Prediction
explanation = explainer.explain_instance(
    image_np.astype('uint8'),
    predict_fn,
    top_labels=2,
    hide_color=0,
    num_samples=1000  # you can increase for more accurate explanations
)
#Visualize the Explanation
from skimage.color import label2rgb

# Choose label index (e.g., 0 = Human, 1 = AI — adjust based on your dataset)
label_index = explanation.top_labels[0]  # or manually use 0 or 1

temp, mask = explanation.get_image_and_mask(
    label_index,
    positive_only=True,
    num_features=10,
    hide_rest=False
)

plt.figure(figsize=(6, 6))
plt.title('LIME Explanation')
plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.axis('off')
plt.show()

from google.colab import files
uploaded = files.upload()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
import cv2

#Load and Preprocess the Image
img_path = "adachi-ginko_90.jpg"
def get_img_array(img_path, size=(224, 224)):
    img = image.load_img(img_path, target_size=size)
    array = image.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    array = tf.keras.applications.densenet.preprocess_input(array)
    return array, img

img_array, original_img = get_img_array(img_path)

#Grad-CAM Function
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model(
        inputs=[model.inputs],
        outputs=[model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()
#Apply Grad-CAM on Xception Branch
# Name of last conv layer in Xception branch
last_conv_xception = 'conv2d'  # Assuming it's named exactly like that after xception.output

heatmap_x = make_gradcam_heatmap(img_array, model, last_conv_xception)
#Apply Grad-CAM on DenseNet201 Branch
last_conv_densenet = 'conv2d_1'  # Adjust name if needed

heatmap_y = make_gradcam_heatmap(img_array, model, last_conv_densenet)

#Superimpose Heatmap on Image
def superimpose_heatmap(heatmap, original_image, alpha=0.4, colormap=cv2.COLORMAP_JET):
    img = np.array(original_image)
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap_color = cv2.applyColorMap(heatmap, colormap)
    superimposed_img = heatmap_color * alpha + img
    return np.uint8(superimposed_img)

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.title("Grad-CAM: Xception")
plt.imshow(superimpose_heatmap(heatmap_x, original_img))
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Grad-CAM: DenseNet201")
plt.imshow(superimpose_heatmap(heatmap_y, original_img))
plt.axis('off')

plt.tight_layout()
plt.show()

!pip install lime

from lime import lime_image
from skimage.segmentation import mark_boundaries
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
img_path = 'adachi-ginko_90.jpg'

def preprocess_lime(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_np = image.img_to_array(img).astype('double')
    return img_np

image_np = preprocess_lime(img_path)

#Define a LIME-Compatible Prediction Function
def predict_fn(images):
    # Apply the same preprocessing as used for DenseNet/Xception
    images = tf.keras.applications.densenet.preprocess_input(images.copy())
    return model.predict(images)

#Create LIME Image Explainer
explainer = lime_image.LimeImageExplainer()
#Explain a Prediction
explanation = explainer.explain_instance(
    image_np.astype('uint8'),
    predict_fn,
    top_labels=2,
    hide_color=0,
    num_samples=1000  # you can increase for more accurate explanations
)
#Visualize the Explanation
from skimage.color import label2rgb

# Choose label index (e.g., 0 = Human, 1 = AI — adjust based on your dataset)
label_index = explanation.top_labels[0]  # or manually use 0 or 1

temp, mask = explanation.get_image_and_mask(
    label_index,
    positive_only=True,
    num_features=10,
    hide_rest=False
)

plt.figure(figsize=(6, 6))
plt.title('LIME Explanation')
plt.imshow(mark_boundaries(temp / 255.0, mask))
plt.axis('off')
plt.show()

"""#Applying Multi-Scale Fusion(Cocatenation at different levels)"""

from tensorflow.keras.models import Model
from tensorflow.keras import Input, models
from tensorflow.keras.applications import DenseNet201, Xception
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     Concatenate, Reshape)
from tensorflow.keras.regularizers import l2
from keras.models import Sequential
from keras.layers import Dense, Flatten

# Input
input_layer = Input(shape=(224, 224, 3))

# --- Xception Branch ---
xception = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
xception_out = xception.output  # Final conv layer (7x7x2048 for Xception)

# Optional: Get intermediate layers (e.g., mid-level features)
xception_mid = xception.get_layer("block4_sepconv2_bn").output  # Example layer

# --- DenseNet201 Branch ---
densenet = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
densenet_out = densenet.output  # Final conv layer (7x7x1920 for DenseNet201)

# Optional: Get intermediate layers
densenet_mid = densenet.get_layer("pool3_conv").output  # Example layer

# --- Feature Fusion Strategies ---
# Option A: Concatenate only final features (simplest)
combined = concatenate([
    GlobalAveragePooling2D()(xception_out),
    GlobalAveragePooling2D()(densenet_out)
])

# Option B: Multi-level fusion (better for fine-grained tasks)
xception_gap = GlobalAveragePooling2D()(xception_out)
densenet_gap = GlobalAveragePooling2D()(densenet_out)
xception_mid_gap = GlobalAveragePooling2D()(xception_mid)
densenet_mid_gap = GlobalAveragePooling2D()(densenet_mid)

combined = concatenate([xception_gap, densenet_gap, xception_mid_gap, densenet_mid_gap])

# --- Final Layers ---
z = Dense(512, activation='relu')(combined)  # Increase units due to larger concatenated features
output = Dense(2, activation='softmax')(z)

model = Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#Applying Cross model Attention fusion"""

from tensorflow.keras.models import Model
from tensorflow.keras import Input, models
from tensorflow.keras.applications import DenseNet201, Xception
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     concatenate, Reshape)
from tensorflow.keras.regularizers import l2
from keras.models import Sequential
from keras.layers import Dense, Flatten
from tensorflow.keras.layers import Multiply, Dense, Reshape

input_layer = Input(shape=(224, 224, 3))

# --- Xception Branch ---
xception = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
xception_out = xception.output  # Final conv layer (7x7x2048 for Xception)

# --- DenseNet201 Branch ---
densenet = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
densenet_out = densenet.output  # Final conv layer (7x7x1920 for DenseNet201)


# After getting xception_out and densenet_out (before GAP)
xception_att = Conv2D(1, (1, 1), activation='sigmoid')(xception_out)  # Attention map
weighted_densenet = Multiply()([densenet_out, xception_att])  # DenseNet features weighted by Xception's attention

# Now concatenate
combined = concatenate([
    GlobalAveragePooling2D()(xception_out),
    GlobalAveragePooling2D()(weighted_densenet)
])

# Proceed with final layers...
z = Dense(512, activation='relu')(combined)  # Increase units due to larger concatenated features
output = Dense(2, activation='softmax')(z)

model = Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))

"""#Applying Feature Reduction Techniques Variance Threshold, PCA, K-Best"""

from tensorflow.keras.layers import Layer, concatenate
from tensorflow.keras import Input, models
from tensorflow.keras.applications import Xception, DenseNet201
import tensorflow as tf
from tensorflow.keras.layers import (Conv2D, BatchNormalization, Dense, Dropout, Input,
                                     GlobalAveragePooling2D, GlobalMaxPooling2D, Multiply,
                                     Concatenate, Reshape)
from tensorflow.keras import backend as K
from tensorflow.keras.regularizers import l2
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.layers import Input

# Squeeze-and-Excitation (SE) Block
def squeeze_excite_block(input_tensor, ratio=16):
    filters = input_tensor.shape[-1]
    se = GlobalAveragePooling2D()(input_tensor)
    se = Dense(filters // ratio, activation='relu', kernel_regularizer=l2(0.001))(se)
    se = Dense(filters, activation='sigmoid', kernel_regularizer=l2(0.001))(se)
    se = Reshape((1, 1, filters))(se)
    return Multiply()([input_tensor, se])

# Spatial Attention Block
def spatial_attention_block(input_tensor):
    avg_pool = GlobalAveragePooling2D(keepdims=True)(input_tensor)
    max_pool = GlobalMaxPooling2D(keepdims=True)(input_tensor)
    concat = Concatenate(axis=-1)([avg_pool, max_pool])
    attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(concat)
    return Multiply()([input_tensor, attention])


# Custom feature selection layer (PCA-like)
class FeatureReduction(Layer):
    def __init__(self, n_components, **kwargs):
        super(FeatureReduction, self).__init__(**kwargs)
        self.n_components = n_components

    def build(self, input_shape):
        # Learnable projection matrix
        self.projection = self.add_weight(
            name='projection',
            shape=(input_shape[-1], self.n_components),
            initializer='orthogonal',
            trainable=True)
        super().build(input_shape)

    def call(self, inputs):
        # Apply projection (similar to PCA but learned end-to-end)
        return tf.matmul(inputs, self.projection)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.n_components)

# Input layer
input_layer = Input(shape=(224, 224, 3))

# Xception branch
xception = Xception(weights='imagenet', include_top=False, input_tensor=input_layer)
xception.trainable = True
for layer in xception.layers[:100]:
    layer.trainable = False
x = xception.output
x = Conv2D(32, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = squeeze_excite_block(x)
x = spatial_attention_block(x)
x = GlobalAveragePooling2D()(x)

# DenseNet201 branch
densenet = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_layer)
densenet.trainable = True
for layer in densenet.layers[:100]:
    layer.trainable = False
y = densenet.output
y = Conv2D(32, (3, 3), padding='same')(y)
y = BatchNormalization()(y)
y = squeeze_excite_block(y)
y = spatial_attention_block(y)
y = GlobalAveragePooling2D()(y)

# Concatenate features from both models
combined = concatenate([x, y])

# Feature reduction techniques
# Option 1: Variance Threshold-like (learned)
z = FeatureReduction(n_components=256)(combined)  # Reduces to 256 features

# Option 2: Add dropout for implicit feature selection
z = Dropout(0.5)(combined)

# Option 3: Bottleneck dense layer
# z = Dense(256, activation='relu', kernel_regularizer='l1')(combined)  # L1 regularization for sparsity

# Final layers
z = Dense(256, activation='relu')(z)
output = Dense(2, activation='softmax')(z)

# Create the combined model
model = models.Model(inputs=input_layer, outputs=output)
model.summary()

from keras.callbacks import ModelCheckpoint

# Define the file path where the model will be saved
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)

# Compile the model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model for the specified number of epochs
history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[model_checkpoint])

loss, accuracy = model.evaluate(test_dataset)
print("Test Accuracy: {:.2f}%".format(accuracy * 100))